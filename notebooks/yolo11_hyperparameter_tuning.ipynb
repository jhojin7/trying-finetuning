{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO11 Hyperparameter Tuning\n",
    "\n",
    "**Enhanced with Automated Hyperparameter Optimization**\n",
    "\n",
    "Based on:\n",
    "- [Ultralytics Hyperparameter Tuning Guide](https://docs.ultralytics.com/guides/hyperparameter-tuning/)\n",
    "- [Ray Tune Integration](https://docs.ultralytics.com/integrations/ray-tune/)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates automated hyperparameter tuning for YOLO11 models using:\n",
    "1. **Built-in Genetic Algorithm Tuner** - Simple, no extra dependencies\n",
    "2. **Ray Tune Integration** - Advanced optimization with parallel execution\n",
    "\n",
    "### What Gets Tuned?\n",
    "\n",
    "The tuner optimizes:\n",
    "- **Learning rate**: `lr0`, `lrf`\n",
    "- **Optimizer**: `momentum`, `weight_decay`\n",
    "- **Loss weights**: `box`, `cls`, `dfl`\n",
    "- **Augmentation**: HSV, rotation, translation, scale, shear, perspective, flip, mosaic, mixup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Check if we're in a notebook subdirectory\n",
    "if Path.cwd().name == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "Install Ultralytics and optionally Ray Tune for advanced optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic installation\n",
    "!pip install -q ultralytics\n",
    "\n",
    "# Optional: Install Ray Tune for advanced optimization\n",
    "# Uncomment the line below to enable Ray Tune\n",
    "# !pip install -q \"ray[tune]\" wandb\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Installation Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Reuse the dataset preparation from the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(datapath, train_pct=0.9):\n",
    "    \"\"\"\n",
    "    Split dataset into train and validation folders.\n",
    "    \n",
    "    Args:\n",
    "        datapath: Path to custom_data folder containing images/ and labels/\n",
    "        train_pct: Percentage of data to use for training (default 0.9)\n",
    "    \"\"\"\n",
    "    datapath = Path(datapath)\n",
    "    images_dir = datapath / 'images'\n",
    "    labels_dir = datapath / 'labels'\n",
    "    \n",
    "    if not images_dir.exists():\n",
    "        print(f\"❌ Error: {images_dir} does not exist!\")\n",
    "        return\n",
    "    \n",
    "    # Create output directories\n",
    "    data_root = Path('data')\n",
    "    for split in ['train', 'validation']:\n",
    "        (data_root / split / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (data_root / split / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpeg'))\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "    \n",
    "    # Shuffle and split\n",
    "    random.shuffle(image_files)\n",
    "    split_idx = int(len(image_files) * train_pct)\n",
    "    train_files = image_files[:split_idx]\n",
    "    val_files = image_files[split_idx:]\n",
    "    \n",
    "    print(f\"Train: {len(train_files)} images\")\n",
    "    print(f\"Validation: {len(val_files)} images\")\n",
    "    \n",
    "    # Copy files\n",
    "    for file_list, split in [(train_files, 'train'), (val_files, 'validation')]:\n",
    "        for img_file in file_list:\n",
    "            # Copy image\n",
    "            shutil.copy2(img_file, data_root / split / 'images' / img_file.name)\n",
    "            \n",
    "            # Copy corresponding label\n",
    "            label_file = labels_dir / f\"{img_file.stem}.txt\"\n",
    "            if label_file.exists():\n",
    "                shutil.copy2(label_file, data_root / split / 'labels' / label_file.name)\n",
    "    \n",
    "    print(\"\\n✅ Dataset split complete!\")\n",
    "    print(f\"Output directory: {data_root.absolute()}\")\n",
    "\n",
    "# Only run if dataset hasn't been split yet\n",
    "if not Path('data/train').exists():\n",
    "    train_val_split('custom_data', train_pct=0.9)\n",
    "else:\n",
    "    print(\"✅ Dataset already split!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n",
    "    \"\"\"\n",
    "    Create YOLO data.yaml configuration file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path_to_classes_txt):\n",
    "        print(f'❌ classes.txt not found at {path_to_classes_txt}')\n",
    "        return None\n",
    "    \n",
    "    with open(path_to_classes_txt, 'r') as f:\n",
    "        classes = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    \n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    # Create data dictionary\n",
    "    data = {\n",
    "        'path': str(Path('data').absolute()),\n",
    "        'train': 'train/images',\n",
    "        'val': 'validation/images',\n",
    "        'nc': num_classes,\n",
    "        'names': classes\n",
    "    }\n",
    "    \n",
    "    # Write YAML file\n",
    "    with open(path_to_data_yaml, 'w') as f:\n",
    "        yaml.dump(data, f, sort_keys=False)\n",
    "    \n",
    "    print(f'✅ Created config file at {path_to_data_yaml}')\n",
    "    print(f'Number of classes: {num_classes}')\n",
    "    print(f'Classes: {classes}')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create YAML config\n",
    "classes_txt_path = 'custom_data/classes.txt'\n",
    "data_yaml_path = 'data.yaml'\n",
    "\n",
    "if not Path(data_yaml_path).exists():\n",
    "    config = create_data_yaml(classes_txt_path, data_yaml_path)\n",
    "else:\n",
    "    print(f\"✅ {data_yaml_path} already exists!\")\n",
    "    with open(data_yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"Classes: {config['names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning Configuration\n",
    "\n",
    "### Choose Your Tuning Method\n",
    "\n",
    "**Method 1: Built-in Genetic Algorithm** (Recommended for beginners)\n",
    "- No extra dependencies\n",
    "- Uses mutation and crossover\n",
    "- Simple to use\n",
    "- Good for 30-300 iterations\n",
    "\n",
    "**Method 2: Ray Tune** (Advanced users)\n",
    "- Requires `ray[tune]` installation\n",
    "- Bayesian optimization, Hyperband\n",
    "- Parallel execution\n",
    "- Better for large search spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HYPERPARAMETER TUNING CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Choose tuning method: 'genetic' or 'ray_tune'\n",
    "TUNING_METHOD = 'genetic'  # Change to 'ray_tune' for Ray Tune\n",
    "\n",
    "# Base configuration\n",
    "BASE_CONFIG = {\n",
    "    'data': 'data.yaml',\n",
    "    'model': 'yolo11s.pt',    # Start with small model for faster tuning\n",
    "    'epochs': 30,              # Epochs per iteration\n",
    "    'imgsz': 640,\n",
    "    'batch': -1,               # Auto batch size\n",
    "    'project': 'runs/tune',\n",
    "    'plots': False,            # Disable plots during tuning to save time\n",
    "    'save': False,             # Only save best model\n",
    "    'val': True,               # Validate after each iteration\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    BASE_CONFIG['device'] = 0\n",
    "elif torch.backends.mps.is_available():\n",
    "    BASE_CONFIG['device'] = 'mps'\n",
    "else:\n",
    "    BASE_CONFIG['device'] = 'cpu'\n",
    "\n",
    "# Genetic Algorithm Tuning Settings\n",
    "GENETIC_CONFIG = {\n",
    "    'iterations': 30,          # Number of tuning iterations (10-300 recommended)\n",
    "    'optimizer': 'AdamW',      # Optimizer to use\n",
    "}\n",
    "\n",
    "# Ray Tune Settings (only used if TUNING_METHOD='ray_tune')\n",
    "RAY_TUNE_CONFIG = {\n",
    "    'use_ray': True,\n",
    "    'iterations': 20,          # Number of trials\n",
    "    'grace_period': 10,        # Min epochs before early stopping\n",
    "    'gpu_per_trial': 1 if torch.cuda.is_available() else 0,\n",
    "}\n",
    "\n",
    "# Custom search space (optional)\n",
    "# Uncomment and modify to customize hyperparameter ranges\n",
    "CUSTOM_SEARCH_SPACE = {\n",
    "    # Learning rate\n",
    "    # 'lr0': (1e-5, 1e-1),       # Initial learning rate\n",
    "    # 'lrf': (0.01, 1.0),        # Final learning rate fraction\n",
    "    \n",
    "    # Optimizer\n",
    "    # 'momentum': (0.6, 0.98),   # SGD momentum\n",
    "    # 'weight_decay': (0.0, 0.001),  # Optimizer weight decay\n",
    "    \n",
    "    # Augmentation\n",
    "    # 'hsv_h': (0.0, 0.1),       # HSV-Hue augmentation\n",
    "    # 'hsv_s': (0.0, 0.9),       # HSV-Saturation augmentation\n",
    "    # 'hsv_v': (0.0, 0.9),       # HSV-Value augmentation\n",
    "    # 'degrees': (0.0, 45.0),    # Rotation degrees\n",
    "    # 'translate': (0.0, 0.9),   # Translation fraction\n",
    "    # 'scale': (0.0, 0.9),       # Scaling factor\n",
    "    # 'shear': (0.0, 10.0),      # Shear degrees\n",
    "    # 'perspective': (0.0, 0.001),  # Perspective transform\n",
    "    # 'flipud': (0.0, 1.0),      # Vertical flip probability\n",
    "    # 'fliplr': (0.0, 1.0),      # Horizontal flip probability\n",
    "    # 'mosaic': (0.0, 1.0),      # Mosaic augmentation probability\n",
    "    # 'mixup': (0.0, 1.0),       # Mixup augmentation probability\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Tuning Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Method: {TUNING_METHOD.upper()}\")\n",
    "print(f\"Model: {BASE_CONFIG['model']}\")\n",
    "print(f\"Epochs per iteration: {BASE_CONFIG['epochs']}\")\n",
    "print(f\"Device: {BASE_CONFIG['device']}\")\n",
    "if TUNING_METHOD == 'genetic':\n",
    "    print(f\"Iterations: {GENETIC_CONFIG['iterations']}\")\n",
    "    print(f\"Total training runs: {GENETIC_CONFIG['iterations']}\")\n",
    "else:\n",
    "    print(f\"Trials: {RAY_TUNE_CONFIG['iterations']}\")\n",
    "print(f\"Custom search space: {'Yes' if CUSTOM_SEARCH_SPACE else 'No (using defaults)'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Hyperparameter Tuning\n",
    "\n",
    "This will:\n",
    "1. Initialize the model\n",
    "2. Run multiple training iterations with different hyperparameters\n",
    "3. Track the best configuration\n",
    "4. Save results to CSV/logs\n",
    "\n",
    "**Note:** This can take several hours depending on:\n",
    "- Number of iterations\n",
    "- Epochs per iteration\n",
    "- Dataset size\n",
    "- Hardware (GPU recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = YOLO(BASE_CONFIG['model'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"🚀 Starting Hyperparameter Tuning ({TUNING_METHOD.upper()})\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThis may take several hours...\\n\")\n",
    "\n",
    "# Merge configurations\n",
    "tune_kwargs = BASE_CONFIG.copy()\n",
    "\n",
    "if TUNING_METHOD == 'genetic':\n",
    "    # Built-in genetic algorithm tuning\n",
    "    tune_kwargs.update(GENETIC_CONFIG)\n",
    "    if CUSTOM_SEARCH_SPACE:\n",
    "        tune_kwargs['space'] = CUSTOM_SEARCH_SPACE\n",
    "    \n",
    "    # Run tuning\n",
    "    results = model.tune(**tune_kwargs)\n",
    "    \n",
    "elif TUNING_METHOD == 'ray_tune':\n",
    "    # Ray Tune optimization\n",
    "    try:\n",
    "        from ray import tune as ray_tune_module\n",
    "        \n",
    "        tune_kwargs.update(RAY_TUNE_CONFIG)\n",
    "        \n",
    "        # Convert custom search space to Ray Tune format if provided\n",
    "        if CUSTOM_SEARCH_SPACE:\n",
    "            ray_space = {}\n",
    "            for key, (low, high) in CUSTOM_SEARCH_SPACE.items():\n",
    "                ray_space[key] = ray_tune_module.uniform(low, high)\n",
    "            tune_kwargs['space'] = ray_space\n",
    "        \n",
    "        # Run Ray Tune\n",
    "        results = model.tune(**tune_kwargs)\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ Ray Tune not installed!\")\n",
    "        print(\"Run: pip install 'ray[tune]'\")\n",
    "        print(\"Falling back to genetic algorithm...\")\n",
    "        tune_kwargs.update(GENETIC_CONFIG)\n",
    "        results = model.tune(**tune_kwargs)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Hyperparameter Tuning Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Tuning Results\n",
    "\n",
    "Review the evolution of hyperparameters and fitness scores across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tuning results\n",
    "tune_results_csv = Path('runs/tune/tune/evolve.csv')\n",
    "\n",
    "if tune_results_csv.exists():\n",
    "    df = pd.read_csv(tune_results_csv)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TUNING RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Show best iteration\n",
    "    best_idx = df['fitness'].idxmax()\n",
    "    best_fitness = df.loc[best_idx, 'fitness']\n",
    "    \n",
    "    print(f\"\\nBest iteration: {best_idx + 1}\")\n",
    "    print(f\"Best fitness: {best_fitness:.4f}\")\n",
    "    \n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Display key hyperparameters\n",
    "    key_params = ['lr0', 'lrf', 'momentum', 'weight_decay', 'box', 'cls', 'dfl',\n",
    "                  'hsv_h', 'hsv_s', 'hsv_v', 'degrees', 'translate', 'scale',\n",
    "                  'shear', 'perspective', 'flipud', 'fliplr', 'mosaic', 'mixup']\n",
    "    \n",
    "    for param in key_params:\n",
    "        if param in df.columns:\n",
    "            value = df.loc[best_idx, param]\n",
    "            print(f\"  {param:15s}: {value:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Show evolution statistics\n",
    "    print(\"\\nEvolution Statistics:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total iterations: {len(df)}\")\n",
    "    print(f\"Initial fitness: {df['fitness'].iloc[0]:.4f}\")\n",
    "    print(f\"Final fitness: {df['fitness'].iloc[-1]:.4f}\")\n",
    "    print(f\"Best fitness: {best_fitness:.4f}\")\n",
    "    print(f\"Improvement: {(best_fitness - df['fitness'].iloc[0]) / df['fitness'].iloc[0] * 100:.2f}%\")\n",
    "    \n",
    "    # Plot fitness evolution\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Subplot 1: Fitness over iterations\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(df.index + 1, df['fitness'], marker='o', linewidth=2, markersize=4)\n",
    "    plt.axhline(y=best_fitness, color='r', linestyle='--', label=f'Best: {best_fitness:.4f}')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Fitness')\n",
    "    plt.title('Fitness Evolution Over Iterations', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Key metrics evolution\n",
    "    plt.subplot(3, 1, 2)\n",
    "    metrics = ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
    "    available_metrics = [m for m in metrics if m in df.columns]\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        plt.plot(df.index + 1, df[metric], marker='o', linewidth=2, markersize=3, label=metric.split('/')[-1])\n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metrics Evolution', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Learning rate evolution\n",
    "    plt.subplot(3, 1, 3)\n",
    "    if 'lr0' in df.columns:\n",
    "        plt.plot(df.index + 1, df['lr0'], marker='o', linewidth=2, markersize=4, label='lr0', color='orange')\n",
    "    if 'lrf' in df.columns:\n",
    "        plt.plot(df.index + 1, df['lrf'], marker='s', linewidth=2, markersize=4, label='lrf', color='green')\n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Learning Rate Evolution', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('runs/tune/tune/evolution_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✅ Evolution plot saved to: runs/tune/tune/evolution_analysis.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Tuning results not found. Make sure tuning completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Final Model with Best Hyperparameters\n",
    "\n",
    "Now train a full model using the best hyperparameters discovered during tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best hyperparameters\n",
    "if tune_results_csv.exists():\n",
    "    df = pd.read_csv(tune_results_csv)\n",
    "    best_idx = df['fitness'].idxmax()\n",
    "    \n",
    "    # Extract best hyperparameters\n",
    "    best_params = {}\n",
    "    for col in df.columns:\n",
    "        if col not in ['fitness', 'metrics/precision(B)', 'metrics/recall(B)', \n",
    "                       'metrics/mAP50(B)', 'metrics/mAP50-95(B)']:\n",
    "            best_params[col] = df.loc[best_idx, col]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🚀 Training Final Model with Best Hyperparameters\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Final training configuration\n",
    "    FINAL_CONFIG = {\n",
    "        'data': 'data.yaml',\n",
    "        'model': 'yolo11s.pt',  # Can upgrade to 'yolo11m.pt' or 'yolo11l.pt' for better accuracy\n",
    "        'epochs': 100,           # More epochs for final training\n",
    "        'imgsz': 640,\n",
    "        'batch': -1,\n",
    "        'device': BASE_CONFIG['device'],\n",
    "        'project': 'runs/detect',\n",
    "        'name': 'final_tuned',\n",
    "        'exist_ok': True,\n",
    "        'patience': 50,\n",
    "        'save': True,\n",
    "        'plots': True,\n",
    "    }\n",
    "    \n",
    "    # Merge best hyperparameters\n",
    "    FINAL_CONFIG.update(best_params)\n",
    "    \n",
    "    # Initialize fresh model\n",
    "    final_model = YOLO(FINAL_CONFIG['model'])\n",
    "    \n",
    "    # Train with best hyperparameters\n",
    "    final_results = final_model.train(**FINAL_CONFIG)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✅ Final Model Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBest weights saved at: runs/detect/final_tuned/weights/best.pt\")\n",
    "    print(f\"Training plots saved at: runs/detect/final_tuned/results.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Cannot find best hyperparameters. Run tuning first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final training results\n",
    "results_plot = Path('runs/detect/final_tuned/results.png')\n",
    "if results_plot.exists():\n",
    "    img = plt.imread(str(results_plot))\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Final Training Results (Tuned Model)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Results plot not found\")\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix = Path('runs/detect/final_tuned/confusion_matrix.png')\n",
    "if confusion_matrix.exists():\n",
    "    img = plt.imread(str(confusion_matrix))\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix (Tuned Model)', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ Confusion matrix not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validate Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best tuned model\n",
    "best_model_path = 'runs/detect/final_tuned/weights/best.pt'\n",
    "\n",
    "if Path(best_model_path).exists():\n",
    "    tuned_model = YOLO(best_model_path)\n",
    "    \n",
    "    print(\"Running validation on tuned model...\\n\")\n",
    "    \n",
    "    # Validate\n",
    "    metrics = tuned_model.val(data='data.yaml')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TUNED MODEL VALIDATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"mAP50:     {metrics.box.map50:.4f} ({metrics.box.map50*100:.2f}%)\")\n",
    "    print(f\"mAP50-95:  {metrics.box.map:.4f} ({metrics.box.map*100:.2f}%)\")\n",
    "    print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "    print(f\"Recall:    {metrics.box.mr:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(f\"⚠️ Model not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare: Baseline vs Tuned Model\n",
    "\n",
    "Compare performance between a baseline model and your tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model metrics if available\n",
    "baseline_results = Path('runs/detect/train/results.csv')\n",
    "tuned_results = Path('runs/detect/final_tuned/results.csv')\n",
    "\n",
    "if baseline_results.exists() and tuned_results.exists():\n",
    "    baseline_df = pd.read_csv(baseline_results)\n",
    "    tuned_df = pd.read_csv(tuned_results)\n",
    "    \n",
    "    # Strip whitespace from column names\n",
    "    baseline_df.columns = baseline_df.columns.str.strip()\n",
    "    tuned_df.columns = tuned_df.columns.str.strip()\n",
    "    \n",
    "    # Get final metrics\n",
    "    metrics_to_compare = ['metrics/precision(B)', 'metrics/recall(B)', \n",
    "                          'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BASELINE vs TUNED MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison_data = []\n",
    "    for metric in metrics_to_compare:\n",
    "        if metric in baseline_df.columns and metric in tuned_df.columns:\n",
    "            baseline_val = baseline_df[metric].iloc[-1]\n",
    "            tuned_val = tuned_df[metric].iloc[-1]\n",
    "            improvement = ((tuned_val - baseline_val) / baseline_val * 100)\n",
    "            \n",
    "            metric_name = metric.split('/')[-1].replace('(B)', '')\n",
    "            print(f\"\\n{metric_name}:\")\n",
    "            print(f\"  Baseline: {baseline_val:.4f}\")\n",
    "            print(f\"  Tuned:    {tuned_val:.4f}\")\n",
    "            print(f\"  Change:   {improvement:+.2f}%\")\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Metric': metric_name,\n",
    "                'Baseline': baseline_val,\n",
    "                'Tuned': tuned_val,\n",
    "                'Improvement': improvement\n",
    "            })\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    if comparison_data:\n",
    "        comp_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Bar chart comparison\n",
    "        x = np.arange(len(comp_df))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0].bar(x - width/2, comp_df['Baseline'], width, label='Baseline', alpha=0.8)\n",
    "        axes[0].bar(x + width/2, comp_df['Tuned'], width, label='Tuned', alpha=0.8)\n",
    "        axes[0].set_xlabel('Metric')\n",
    "        axes[0].set_ylabel('Score')\n",
    "        axes[0].set_title('Baseline vs Tuned Model Performance', fontweight='bold')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(comp_df['Metric'], rotation=45, ha='right')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Improvement chart\n",
    "        colors = ['green' if x > 0 else 'red' for x in comp_df['Improvement']]\n",
    "        axes[1].barh(comp_df['Metric'], comp_df['Improvement'], color=colors, alpha=0.8)\n",
    "        axes[1].set_xlabel('Improvement (%)')\n",
    "        axes[1].set_title('Performance Improvement', fontweight='bold')\n",
    "        axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        axes[1].grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('runs/tune/baseline_vs_tuned_comparison.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n✅ Comparison plot saved to: runs/tune/baseline_vs_tuned_comparison.png\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ Baseline or tuned results not found for comparison.\")\n",
    "    print(\"Run the original training notebook first to create a baseline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(best_model_path).exists():\n",
    "    tuned_model = YOLO(best_model_path)\n",
    "    \n",
    "    # Export to ONNX format\n",
    "    print(\"Exporting tuned model to ONNX format...\")\n",
    "    onnx_path = tuned_model.export(format='onnx', dynamic=True)\n",
    "    print(f\"✅ ONNX model exported to: {onnx_path}\")\n",
    "    \n",
    "    # Uncomment to export to other formats:\n",
    "    # coreml_path = tuned_model.export(format='coreml')  # For iOS/macOS\n",
    "    # tflite_path = tuned_model.export(format='tflite')  # For mobile/edge devices\n",
    "else:\n",
    "    print(f\"⚠️ Model not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Recommendations\n",
    "\n",
    "### What We Did\n",
    "\n",
    "1. **Automated Hyperparameter Search**: Used genetic algorithm or Ray Tune to find optimal hyperparameters\n",
    "2. **Evolution Tracking**: Monitored fitness and metrics across iterations\n",
    "3. **Final Training**: Trained a full model with best hyperparameters\n",
    "4. **Performance Comparison**: Compared baseline vs tuned model\n",
    "\n",
    "### Key Files\n",
    "\n",
    "- **Tuning results**: `runs/tune/tune/evolve.csv`\n",
    "- **Best tuned model**: `runs/detect/final_tuned/weights/best.pt`\n",
    "- **Training plots**: `runs/detect/final_tuned/results.png`\n",
    "- **ONNX export**: `runs/detect/final_tuned/weights/best.onnx`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Further Tuning**:\n",
    "   - Increase iterations (100-300) for better results\n",
    "   - Try Ray Tune with Bayesian optimization\n",
    "   - Customize search space for specific parameters\n",
    "\n",
    "2. **Model Improvements**:\n",
    "   - Use larger model (`yolo11m.pt`, `yolo11l.pt`) for higher accuracy\n",
    "   - Increase final training epochs (150-200)\n",
    "   - Add more training data if possible\n",
    "\n",
    "3. **Advanced Techniques**:\n",
    "   - Enable MongoDB for distributed tuning across machines\n",
    "   - Use ensemble methods with multiple tuned models\n",
    "   - Fine-tune with transfer learning from domain-specific weights\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Ultralytics Hyperparameter Tuning Guide](https://docs.ultralytics.com/guides/hyperparameter-tuning/)\n",
    "- [Ray Tune Integration](https://docs.ultralytics.com/integrations/ray-tune/)\n",
    "- [Ultralytics Documentation](https://docs.ultralytics.com/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy tuning!** 🎯🔧"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
