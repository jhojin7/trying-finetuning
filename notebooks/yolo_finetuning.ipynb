{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Fine-tuning with MPS Support (MacBook M2)\n",
    "\n",
    "This notebook demonstrates fine-tuning YOLOv8 on the electronic components dataset using MPS (Metal Performance Shaders) for Apple Silicon.\n",
    "\n",
    "## Overview\n",
    "- **Dataset**: Electronic Components Detection (COCO format)\n",
    "- **Model**: YOLOv8 (Ultralytics)\n",
    "- **Device**: MPS (Apple Silicon M2)\n",
    "- **Task**: Object Detection\n",
    "\n",
    "## Requirements\n",
    "```bash\n",
    "pip install ultralytics opencv-python pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Paths and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATASET_ROOT = PROJECT_ROOT / 'dataset'\n",
    "YOLO_DATASET_ROOT = PROJECT_ROOT / 'dataset_yolo'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'yolo'\n",
    "MODEL_DIR = OUTPUT_DIR / 'models'\n",
    "VIZ_DIR = OUTPUT_DIR / 'visualizations'\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [OUTPUT_DIR, MODEL_DIR, VIZ_DIR, YOLO_DATASET_ROOT]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"✅ Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"✅ Using CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"⚠️  Using CPU (slower)\")\n",
    "\n",
    "print(f\"\\nProject root: {PROJECT_ROOT}\")\n",
    "print(f\"COCO dataset: {DATASET_ROOT}\")\n",
    "print(f\"YOLO dataset: {YOLO_DATASET_ROOT}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert COCO Format to YOLO Format\n",
    "\n",
    "YOLO requires a specific format:\n",
    "- Images in `images/train`, `images/val`, `images/test`\n",
    "- Labels in `labels/train`, `labels/val`, `labels/test`\n",
    "- Each label file: `<class_id> <x_center> <y_center> <width> <height>` (normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_to_yolo(coco_json_path, output_dir, split_name):\n",
    "    \"\"\"\n",
    "    Convert COCO format annotations to YOLO format.\n",
    "    \n",
    "    Args:\n",
    "        coco_json_path: Path to COCO JSON annotation file\n",
    "        output_dir: Root directory for YOLO dataset\n",
    "        split_name: 'train', 'val', or 'test'\n",
    "    \"\"\"\n",
    "    # Load COCO annotations\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    # Create output directories\n",
    "    images_dir = output_dir / 'images' / split_name\n",
    "    labels_dir = output_dir / 'labels' / split_name\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Build image id to filename mapping\n",
    "    image_id_to_info = {img['id']: img for img in coco_data['images']}\n",
    "    \n",
    "    # Build category id mapping (COCO might start from 0 or 1, YOLO needs 0-indexed)\n",
    "    categories = sorted(coco_data['categories'], key=lambda x: x['id'])\n",
    "    category_id_to_yolo_id = {cat['id']: idx for idx, cat in enumerate(categories)}\n",
    "    category_names = [cat['name'] for cat in categories]\n",
    "    \n",
    "    # Group annotations by image_id\n",
    "    annotations_by_image = {}\n",
    "    for ann in coco_data['annotations']:\n",
    "        image_id = ann['image_id']\n",
    "        if image_id not in annotations_by_image:\n",
    "            annotations_by_image[image_id] = []\n",
    "        annotations_by_image[image_id].append(ann)\n",
    "    \n",
    "    # Convert each image\n",
    "    source_image_dir = Path(coco_json_path).parent\n",
    "    converted_count = 0\n",
    "    \n",
    "    for image_id, image_info in image_id_to_info.items():\n",
    "        image_filename = image_info['file_name']\n",
    "        image_width = image_info['width']\n",
    "        image_height = image_info['height']\n",
    "        \n",
    "        # Copy image\n",
    "        source_image_path = source_image_dir / image_filename\n",
    "        dest_image_path = images_dir / image_filename\n",
    "        \n",
    "        if source_image_path.exists():\n",
    "            shutil.copy2(source_image_path, dest_image_path)\n",
    "        else:\n",
    "            print(f\"⚠️  Image not found: {source_image_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert annotations to YOLO format\n",
    "        label_filename = Path(image_filename).stem + '.txt'\n",
    "        label_path = labels_dir / label_filename\n",
    "        \n",
    "        yolo_annotations = []\n",
    "        if image_id in annotations_by_image:\n",
    "            for ann in annotations_by_image[image_id]:\n",
    "                # COCO bbox format: [x_min, y_min, width, height]\n",
    "                x_min, y_min, bbox_width, bbox_height = ann['bbox']\n",
    "                \n",
    "                # Convert to YOLO format: [x_center, y_center, width, height] (normalized)\n",
    "                x_center = (x_min + bbox_width / 2) / image_width\n",
    "                y_center = (y_min + bbox_height / 2) / image_height\n",
    "                norm_width = bbox_width / image_width\n",
    "                norm_height = bbox_height / image_height\n",
    "                \n",
    "                # Get YOLO class id\n",
    "                yolo_class_id = category_id_to_yolo_id[ann['category_id']]\n",
    "                \n",
    "                # YOLO format: class x_center y_center width height\n",
    "                yolo_annotations.append(\n",
    "                    f\"{yolo_class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\"\n",
    "                )\n",
    "        \n",
    "        # Write label file\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write('\\n'.join(yolo_annotations))\n",
    "        \n",
    "        converted_count += 1\n",
    "    \n",
    "    print(f\"✅ Converted {converted_count} images for {split_name} split\")\n",
    "    return category_names\n",
    "\n",
    "# Convert all splits\n",
    "print(\"Converting COCO dataset to YOLO format...\\n\")\n",
    "\n",
    "# Check if COCO dataset exists\n",
    "if not DATASET_ROOT.exists():\n",
    "    print(f\"❌ COCO dataset not found at {DATASET_ROOT}\")\n",
    "    print(\"Please ensure the dataset directory exists with train/valid/test splits.\")\n",
    "else:\n",
    "    # Convert train set\n",
    "    train_json = DATASET_ROOT / 'train' / '_annotations.coco.json'\n",
    "    if train_json.exists():\n",
    "        class_names = convert_coco_to_yolo(train_json, YOLO_DATASET_ROOT, 'train')\n",
    "    else:\n",
    "        print(f\"⚠️  Training annotations not found: {train_json}\")\n",
    "    \n",
    "    # Convert validation set\n",
    "    valid_json = DATASET_ROOT / 'valid' / '_annotations.coco.json'\n",
    "    if valid_json.exists():\n",
    "        convert_coco_to_yolo(valid_json, YOLO_DATASET_ROOT, 'val')\n",
    "    else:\n",
    "        print(f\"⚠️  Validation annotations not found: {valid_json}\")\n",
    "    \n",
    "    # Convert test set\n",
    "    test_json = DATASET_ROOT / 'test' / '_annotations.coco.json'\n",
    "    if test_json.exists():\n",
    "        convert_coco_to_yolo(test_json, YOLO_DATASET_ROOT, 'test')\n",
    "    else:\n",
    "        print(f\"⚠️  Test annotations not found: {test_json}\")\n",
    "    \n",
    "    print(f\"\\n✅ Dataset conversion complete!\")\n",
    "    print(f\"YOLO dataset location: {YOLO_DATASET_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create YOLO Dataset Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YOLO dataset YAML configuration\n",
    "dataset_yaml = {\n",
    "    'path': str(YOLO_DATASET_ROOT.absolute()),  # Dataset root directory\n",
    "    'train': 'images/train',  # Train images (relative to 'path')\n",
    "    'val': 'images/val',      # Val images (relative to 'path')\n",
    "    'test': 'images/test',    # Test images (relative to 'path')\n",
    "    'nc': len(class_names),   # Number of classes\n",
    "    'names': class_names      # Class names\n",
    "}\n",
    "\n",
    "# Save dataset configuration\n",
    "dataset_yaml_path = YOLO_DATASET_ROOT / 'dataset.yaml'\n",
    "with open(dataset_yaml_path, 'w') as f:\n",
    "    yaml.dump(dataset_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"✅ Dataset configuration created\")\n",
    "print(f\"\\nDataset YAML: {dataset_yaml_path}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  - Number of classes: {len(class_names)}\")\n",
    "print(f\"  - Classes: {class_names[:5]}...\" if len(class_names) > 5 else f\"  - Classes: {class_names}\")\n",
    "\n",
    "# Display the YAML content\n",
    "print(f\"\\nDataset YAML content:\")\n",
    "with open(dataset_yaml_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_yolo_sample(image_path, label_path, class_names, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize a YOLO training sample with annotations.\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(str(image_path))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    # Read labels\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = f.readlines()\n",
    "        \n",
    "        # Draw bounding boxes\n",
    "        for label in labels:\n",
    "            parts = label.strip().split()\n",
    "            if len(parts) == 5:\n",
    "                class_id = int(parts[0])\n",
    "                x_center, y_center, width, height = map(float, parts[1:])\n",
    "                \n",
    "                # Convert normalized coordinates to pixel coordinates\n",
    "                x_center *= w\n",
    "                y_center *= h\n",
    "                width *= w\n",
    "                height *= h\n",
    "                \n",
    "                # Calculate box corners\n",
    "                x1 = int(x_center - width / 2)\n",
    "                y1 = int(y_center - height / 2)\n",
    "                x2 = int(x_center + width / 2)\n",
    "                y2 = int(y_center + height / 2)\n",
    "                \n",
    "                # Draw box and label\n",
    "                color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                label_text = class_names[class_id] if class_id < len(class_names) else f\"Class {class_id}\"\n",
    "                cv2.putText(image, label_text, (x1, y1 - 10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    # Display\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Sample: {image_path.name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{image_path.name}\", fontsize=8)\n",
    "\n",
    "# Visualize a few training samples\n",
    "train_images_dir = YOLO_DATASET_ROOT / 'images' / 'train'\n",
    "train_labels_dir = YOLO_DATASET_ROOT / 'labels' / 'train'\n",
    "\n",
    "if train_images_dir.exists():\n",
    "    image_files = sorted(list(train_images_dir.glob('*.jpg')) + list(train_images_dir.glob('*.png')))\n",
    "    \n",
    "    # Show 6 random samples\n",
    "    num_samples = min(6, len(image_files))\n",
    "    sample_indices = np.random.choice(len(image_files), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        image_path = image_files[sample_idx]\n",
    "        label_path = train_labels_dir / (image_path.stem + '.txt')\n",
    "        visualize_yolo_sample(image_path, label_path, class_names, ax=axes[idx])\n",
    "    \n",
    "    plt.suptitle('Training Samples with Ground Truth Annotations', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    viz_path = VIZ_DIR / 'training_samples.png'\n",
    "    plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Training samples visualization saved to: {viz_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"⚠️  Training images directory not found: {train_images_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLOv8 model\n",
    "# Options: yolov8n.pt (nano), yolov8s.pt (small), yolov8m.pt (medium), yolov8l.pt (large), yolov8x.pt (xlarge)\n",
    "# For demo/quick training, use nano or small\n",
    "model_size = 'yolov8n.pt'  # Fastest for demo, use 'yolov8s.pt' or 'yolov8m.pt' for better accuracy\n",
    "\n",
    "print(f\"Loading {model_size} model...\")\n",
    "model = YOLO(model_size)\n",
    "\n",
    "print(f\"✅ Model loaded: {model_size}\")\n",
    "print(f\"\\nModel summary:\")\n",
    "print(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train the Model\n",
    "\n",
    "### Training Configuration:\n",
    "- **epochs**: Number of training epochs (50-100 for demo, 200-300 for production)\n",
    "- **imgsz**: Input image size (640 is standard)\n",
    "- **batch**: Batch size (-1 for auto, or 8/16/32)\n",
    "- **device**: 'mps' for Apple Silicon, 'cuda' for NVIDIA, 'cpu' for CPU\n",
    "- **patience**: Early stopping patience\n",
    "- **optimizer**: SGD, Adam, AdamW, etc.\n",
    "\n",
    "### Quick Demo Settings (5-10 minutes training):\n",
    "- epochs=50\n",
    "- imgsz=640\n",
    "- batch=-1 (auto)\n",
    "\n",
    "### Production Settings (30-60 minutes training):\n",
    "- epochs=200\n",
    "- imgsz=640\n",
    "- batch=16\n",
    "- patience=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = {\n",
    "    'data': str(dataset_yaml_path),  # Path to dataset YAML\n",
    "    'epochs': 50,                     # Number of epochs (50 for quick demo, 200+ for production)\n",
    "    'imgsz': 640,                     # Image size\n",
    "    'batch': -1,                      # Batch size (-1 for auto)\n",
    "    'device': device,                 # Device (mps, cuda, or cpu)\n",
    "    'project': str(OUTPUT_DIR),       # Save directory\n",
    "    'name': 'train',                  # Experiment name\n",
    "    'exist_ok': True,                 # Overwrite existing experiment\n",
    "    'patience': 20,                   # Early stopping patience\n",
    "    'save': True,                     # Save checkpoints\n",
    "    'plots': True,                    # Save training plots\n",
    "    'verbose': True,                  # Verbose output\n",
    "    'optimizer': 'Adam',              # Optimizer (SGD, Adam, AdamW)\n",
    "    'lr0': 0.001,                     # Initial learning rate\n",
    "    'lrf': 0.01,                      # Final learning rate (lr0 * lrf)\n",
    "    'momentum': 0.937,                # Momentum (for SGD)\n",
    "    'weight_decay': 0.0005,           # Weight decay\n",
    "    'warmup_epochs': 3.0,             # Warmup epochs\n",
    "    'warmup_momentum': 0.8,           # Warmup momentum\n",
    "    'box': 7.5,                       # Box loss gain\n",
    "    'cls': 0.5,                       # Classification loss gain\n",
    "    'dfl': 1.5,                       # DFL loss gain\n",
    "    'hsv_h': 0.015,                   # HSV-Hue augmentation\n",
    "    'hsv_s': 0.7,                     # HSV-Saturation augmentation\n",
    "    'hsv_v': 0.4,                     # HSV-Value augmentation\n",
    "    'degrees': 0.0,                   # Rotation augmentation\n",
    "    'translate': 0.1,                 # Translation augmentation\n",
    "    'scale': 0.5,                     # Scale augmentation\n",
    "    'shear': 0.0,                     # Shear augmentation\n",
    "    'perspective': 0.0,               # Perspective augmentation\n",
    "    'flipud': 0.0,                    # Flip up-down augmentation\n",
    "    'fliplr': 0.5,                    # Flip left-right augmentation\n",
    "    'mosaic': 1.0,                    # Mosaic augmentation\n",
    "    'mixup': 0.0,                     # MixUp augmentation\n",
    "}\n",
    "\n",
    "print(\"🚀 Starting training...\\n\")\n",
    "print(\"Training configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    if key not in ['data', 'project']:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n⏰ Training will take approximately 5-10 minutes on M2 (50 epochs)\")\n",
    "print(f\"📊 Results will be saved to: {OUTPUT_DIR / 'train'}\\n\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(**training_config)\n",
    "\n",
    "print(\"\\n✅ Training complete!\")\n",
    "print(f\"\\n📁 Results saved to: {OUTPUT_DIR / 'train'}\")\n",
    "print(f\"📊 Training plots: {OUTPUT_DIR / 'train' / 'results.png'}\")\n",
    "print(f\"💾 Best model: {OUTPUT_DIR / 'train' / 'weights' / 'best.pt'}\")\n",
    "print(f\"💾 Last model: {OUTPUT_DIR / 'train' / 'weights' / 'last.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training results\n",
    "results_plot = OUTPUT_DIR / 'train' / 'results.png'\n",
    "if results_plot.exists():\n",
    "    img = plt.imread(str(results_plot))\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Training Results', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"⚠️  Results plot not found: {results_plot}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix = OUTPUT_DIR / 'train' / 'confusion_matrix.png'\n",
    "if confusion_matrix.exists():\n",
    "    img = plt.imread(str(confusion_matrix))\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title('Confusion Matrix', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"⚠️  Confusion matrix not found: {confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Best Model and Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model_path = OUTPUT_DIR / 'train' / 'weights' / 'best.pt'\n",
    "if best_model_path.exists():\n",
    "    print(f\"Loading best model from: {best_model_path}\")\n",
    "    model = YOLO(str(best_model_path))\n",
    "    print(\"✅ Best model loaded\")\n",
    "    \n",
    "    # Run validation\n",
    "    print(\"\\n🔍 Running validation...\")\n",
    "    val_results = model.val(data=str(dataset_yaml_path), device=device)\n",
    "    \n",
    "    print(\"\\n✅ Validation complete!\")\n",
    "    print(f\"\\nValidation metrics:\")\n",
    "    print(f\"  mAP50: {val_results.box.map50:.4f}\")\n",
    "    print(f\"  mAP50-95: {val_results.box.map:.4f}\")\n",
    "    print(f\"  Precision: {val_results.box.mp:.4f}\")\n",
    "    print(f\"  Recall: {val_results.box.mr:.4f}\")\n",
    "else:\n",
    "    print(f\"⚠️  Best model not found: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Inference on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on test images\n",
    "test_images_dir = YOLO_DATASET_ROOT / 'images' / 'test'\n",
    "\n",
    "if test_images_dir.exists():\n",
    "    print(f\"Running inference on test images...\")\n",
    "    \n",
    "    # Get test images\n",
    "    test_images = sorted(list(test_images_dir.glob('*.jpg')) + list(test_images_dir.glob('*.png')))\n",
    "    print(f\"Found {len(test_images)} test images\")\n",
    "    \n",
    "    # Run predictions\n",
    "    results = model.predict(\n",
    "        source=str(test_images_dir),\n",
    "        conf=0.25,  # Confidence threshold\n",
    "        iou=0.45,   # NMS IoU threshold\n",
    "        device=device,\n",
    "        save=True,  # Save predictions\n",
    "        save_txt=True,  # Save labels\n",
    "        save_conf=True,  # Save confidence scores\n",
    "        project=str(OUTPUT_DIR),\n",
    "        name='test_predictions',\n",
    "        exist_ok=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Inference complete!\")\n",
    "    print(f\"📁 Predictions saved to: {OUTPUT_DIR / 'test_predictions'}\")\n",
    "    \n",
    "    # Visualize a few predictions\n",
    "    num_samples = min(6, len(test_images))\n",
    "    sample_indices = np.random.choice(len(test_images), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        result = results[sample_idx]\n",
    "        img_with_boxes = result.plot()  # Plot boxes on image\n",
    "        img_with_boxes = cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[idx].imshow(img_with_boxes)\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f\"{test_images[sample_idx].name}\\nDetections: {len(result.boxes)}\", fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Test Predictions (Fine-tuned Model)', fontsize=16, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    viz_path = VIZ_DIR / 'test_predictions.png'\n",
    "    plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Test predictions visualization saved to: {viz_path}\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"⚠️  Test images directory not found: {test_images_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Pre-trained vs Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (not fine-tuned)\n",
    "pretrained_model = YOLO(model_size)\n",
    "\n",
    "# Select a test image\n",
    "if test_images_dir.exists() and len(test_images) > 0:\n",
    "    # Pick a random test image\n",
    "    sample_image = test_images[np.random.randint(len(test_images))]\n",
    "    print(f\"Comparing predictions on: {sample_image.name}\")\n",
    "    \n",
    "    # Predict with pre-trained model\n",
    "    pretrained_result = pretrained_model.predict(\n",
    "        source=str(sample_image),\n",
    "        conf=0.25,\n",
    "        device=device,\n",
    "        verbose=False\n",
    "    )[0]\n",
    "    \n",
    "    # Predict with fine-tuned model\n",
    "    finetuned_result = model.predict(\n",
    "        source=str(sample_image),\n",
    "        conf=0.25,\n",
    "        device=device,\n",
    "        verbose=False\n",
    "    )[0]\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Pre-trained\n",
    "    pretrained_img = pretrained_result.plot()\n",
    "    pretrained_img = cv2.cvtColor(pretrained_img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0].imshow(pretrained_img)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(f'Pre-trained Model\\nDetections: {len(pretrained_result.boxes)}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Fine-tuned\n",
    "    finetuned_img = finetuned_result.plot()\n",
    "    finetuned_img = cv2.cvtColor(finetuned_img, cv2.COLOR_BGR2RGB)\n",
    "    axes[1].imshow(finetuned_img)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(f'Fine-tuned Model\\nDetections: {len(finetuned_result.boxes)}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('Before vs After Fine-tuning Comparison', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save comparison\n",
    "    comparison_path = VIZ_DIR / 'before_after_comparison.png'\n",
    "    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✅ Comparison visualization saved to: {comparison_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n📊 Comparison summary:\")\n",
    "    print(f\"  Pre-trained detections: {len(pretrained_result.boxes)}\")\n",
    "    print(f\"  Fine-tuned detections: {len(finetuned_result.boxes)}\")\n",
    "else:\n",
    "    print(f\"⚠️  No test images available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Model (Optional)\n",
    "\n",
    "Export the model to different formats for deployment:\n",
    "- **ONNX**: For cross-platform deployment\n",
    "- **CoreML**: For iOS/macOS deployment\n",
    "- **TorchScript**: For PyTorch production\n",
    "- **TFLite**: For mobile/edge devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX (most common format)\n",
    "print(\"Exporting model to ONNX format...\")\n",
    "onnx_path = model.export(format='onnx', dynamic=True)\n",
    "print(f\"✅ ONNX model exported to: {onnx_path}\")\n",
    "\n",
    "# Uncomment to export to other formats\n",
    "# coreml_path = model.export(format='coreml')  # For iOS/macOS\n",
    "# print(f\"✅ CoreML model exported to: {coreml_path}\")\n",
    "\n",
    "# torchscript_path = model.export(format='torchscript')  # For PyTorch production\n",
    "# print(f\"✅ TorchScript model exported to: {torchscript_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"YOLO FINE-TUNING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n📁 Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\n📊 Key Files:\")\n",
    "print(f\"  - Best model: {OUTPUT_DIR / 'train' / 'weights' / 'best.pt'}\")\n",
    "print(f\"  - Last checkpoint: {OUTPUT_DIR / 'train' / 'weights' / 'last.pt'}\")\n",
    "print(f\"  - Training plots: {OUTPUT_DIR / 'train' / 'results.png'}\")\n",
    "print(f\"  - Test predictions: {OUTPUT_DIR / 'test_predictions'}\")\n",
    "print(f\"  - Visualizations: {VIZ_DIR}\")\n",
    "print(f\"\\n🎨 Visualizations Created:\")\n",
    "print(f\"  - Training samples: {VIZ_DIR / 'training_samples.png'}\")\n",
    "print(f\"  - Test predictions: {VIZ_DIR / 'test_predictions.png'}\")\n",
    "print(f\"  - Before/After comparison: {VIZ_DIR / 'before_after_comparison.png'}\")\n",
    "print(f\"\\n🚀 Next Steps:\")\n",
    "print(f\"  1. Review training metrics in results.png\")\n",
    "print(f\"  2. Analyze confusion matrix for class-specific issues\")\n",
    "print(f\"  3. Run inference on new images using best.pt\")\n",
    "print(f\"  4. For production: increase epochs to 200-300\")\n",
    "print(f\"  5. Export model to desired format (ONNX, CoreML, etc.)\")\n",
    "print(f\"\\n💡 Quick Inference:\")\n",
    "print(f\"  from ultralytics import YOLO\")\n",
    "print(f\"  model = YOLO('{OUTPUT_DIR / 'train' / 'weights' / 'best.pt'}')\")\n",
    "print(f\"  results = model.predict('image.jpg', device='mps')\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
